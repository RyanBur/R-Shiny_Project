---
title: "R-Shiny Dataset Transformation"
#output: html_notebook
---


Importing the libraries I will need and the data I will be manipulating
```{r}
library(dplyr)
library(jsonlite)
```
```{r}
data = read.csv('data\\US_youtube_trending_data.csv', header=TRUE)
categories_data = fromJSON('data\\US_category_id.json', flatten=TRUE)
```

Creating a dataframe with the text version of the categories information for my use.
```{r}
categories_df = data.frame(categories_data$items)
categories_df = categories_df %>% select(number=id, category_text=snippet.title)
categories_df$number = as.integer(categories_df$number)
```
Combine this category text with my original dataframe

```{r}
data
```


```{r}
data = data %>% merge(categories_df, by.x='categoryId', by.y='number') %>% arrange(trending_date)
```
I have only changed one data field, and there are many to go still.


Convert the published and trending datetimes to actual datetimes from characters. 
```{r}
data$publishedAt = data$publishedAt %>% strptime('%Y-%m-%dT%H:%M:%S', tz='UTC')
data$trending_date = data$trending_date %>% strptime('%Y-%m-%dT%H:%M:%S', tz='UTC')
```

Add a column that stores the time between publish and the trending record date.
```{r}
data = data %>% mutate(publish_to_trending_days = round(ifelse((trending_date - publishedAt)>0,difftime(trending_date, publishedAt, units='days'),as.difftime(0,units='days')),2))
```
Rearrange my columns and get rid of a few I won't use at all (categoryId, thumbnail_link, description):
```{r}
data = data %>% select(video_id, title, channelId, channelTitle, publishedAt, trending_date, publish_to_trending_days, category_text, tags, view_count, likes, dislikes, comment_count, comments_disabled, ratings_disabled)
```

26 strange records with 0 views, loooks like google-promoted content that shouldn't really be on the list.
```{r}
data = data %>% filter(view_count!=0)
```

Convert the ratings_disabled and comments_disabled into boolean values.
```{r}
# First, check the only character values are True and False
unique(data$comments_disabled)
unique(data$ratings_disabled)

```
```{r}
# Now convert the columns
data$comments_disabled = as.logical(data$comments_disabled)
data$ratings_disabled = as.logical(data$ratings_disabled)
```

Get rid of the double- entries for the two days in the middle of the dataset that were duped, and the most recent few days that had inconsistent/possibly duped data.
The testing for all this code is in the Exploratory Data Analysis RNotebook
```{r}
good_rows_feb24 = data %>% filter(as.Date(trending_date) == as.Date('2021-02-24')) %>%group_by(video_id) %>% .[!duplicated(.$video_id), ]

good_rows_mar2 = data %>% filter(as.Date(trending_date) == as.Date('2021-03-02')) %>%group_by(video_id) %>% .[!duplicated(.$video_id), ]

data_ex_duped_dates = filter(data, as.Date(trending_date) != as.Date('2021-02-24') & as.Date(trending_date) != as.Date('2021-03-02') & as.Date(trending_date) < '2021-05-31')

data_clean = ungroup(rbind(good_rows_feb24, good_rows_mar2, data_ex_duped_dates)) %>% arrange(trending_date)


#This got rid of 1,391 rows. If things went perfectly, it would have been 1,400 rows. 
cat('Got rid of', dim(data)[1] - dim(data_clean)[1],'rows. If things went perfectly, it would have been 1,400 rows.')

```



save my data as my working dataset.
```{r}
write.csv(data_clean, 'data\\Youtube_Working_Dataset.csv', row.names=FALSE)
```


Dataset was reworked in Python to get a top_tags column.
Details are in the jupyter notebook for this project


Dataset was re-imported, and the columns that are supposed to be logical values wre re-processed. They didnt come through before, Not sure why. 

```{r}
data_NEW = read.csv('data\\Youtube_Working_Dataset_NEW.csv', header=TRUE)
data_NEW$comments_disabled = as.logical(data_NEW$comments_disabled)
data_NEW$ratings_disabled = as.logical(data_NEW$ratings_disabled)
write.csv(data_NEW, 'data\\Youtube_Working_Dataset_NEW.csv', row.names=FALSE)
```


