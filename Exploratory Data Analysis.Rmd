---
title: "Exploratory Data Analysis"
#output: html_notebook
---
```{r}
library(dplyr)
library(ggplot2)
library(stringr)
```

This notebook will focus on Exploratory Data Analysis to help understand my data more and see emerging areas of trends and relationships. 
Load in the processed data:
```{r}
data = read.csv('data\\Youtube_Working_Dataset_NEW.csv', header=TRUE)
data
```
How many videos per day made the trending list?
```{r}
# Quick summary of the number of rows per trending day
summary(data %>% count(trending_date))

# Shows the number of trending days that had each count of row numbers.
data %>% count(trending_date) %>% count(n)
```
Almost all the days covered by the dataset had 199 or 200 entries for that day. The 199 and 197 entry days are close enough to leave as-is, and I fixed the issue of a few days that had 400.

```{r}
dim(data)[1] - dim(distinct(data))[1]
```

How many different videos made the list altogether?
```{r}
unique_vids = dim(distinct(data, video_id))[1]
unique_vids
```
9,835 different videos made the top daily trending videos list in the 280 days covered by the data.
```{r}
dim(data)[1] / unique_vids
```
This means that on average, a video that made the trending list spent 5.69 days trending in the last 280 days.


Now, how many videos were repeat trenders?
```{r}
data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1)
unique_vids - dim(data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1))[1]

```
According to this quick analysis, there are only 376 out of 9,835 videos that made the top daily trending list only once. That in itself is both very surprising and not that surprising... If a video makes the trending list on a day it is very likely to make it again.

Plot the frequency of trending days. I want to see what is a common amount of time to trend for. 
```{r}
trend_days_count = data %>% count(video_id) %>% arrange(desc(n))
trend_days_proportions = data %>% count(video_id) %>% count(n) %>% mutate(proportion=round(nn/unique_vids,3)) %>% select(n, proportion) 

trend_days_proportions %>% ggplot() + geom_col(aes(x=n, y=proportion), fill='darkgoldenrod4') + coord_cartesian(xlim=c(0,15)) + labs(title='Days Spent Trending Per Video', x='Number of Days Trended', y='Proportion of Trending Records')  + theme_bw()  + theme(plot.title = element_text(hjust = 0.5, size=14, face='bold'), panel.background = element_rect(fill = "lightblue", colour = "lightblue", size = 0.5, linetype = "solid"), axis.title=element_text(size=12))


cat('Standard Deviation of days trending:', sd(trend_days_count$n), '\n')
cat('Mean days trending:', mean(trend_days_count$n), '\n')
cat('Median days trending:', median(trend_days_count$n), '\n')
cat('Max days trending:', max(trend_days_count$n), '\n')
```
Different line of questioning: How many creators got multiple videos on the trending list in the 280-day period?
```{r}
vids_per_creator = data %>% group_by(channelId) %>% 
    summarise(number_trending_videos=n_distinct(video_id))
creator_count = dim(vids_per_creator)[1]


vids_per_creator %>% group_by(number_trending_videos) %>% summarise(proportion=n()/creator_count) %>% ggplot() + geom_col(aes(x=number_trending_videos, y=proportion))


cat('Number of creators who made the top daily trending list in 283 days:', creator_count, '\n')
cat('Number of creators who had more than one video make the top trending list:',
    dim(vids_per_creator[vids_per_creator$number_trending_videos > 1,])[1], '\n')
cat('Average videos per creator on list:', mean(vids_per_creator$number_trending_videos), '\n')
cat('Median videos per creator on list:', median(vids_per_creator$number_trending_videos), '\n')

vids_per_creator %>% ggplot() + geom_bar(aes(x=number_trending_videos))
```
About half of all creators who made the list only had 1 video make the top daily trending list during this time period.
The data is very skewed, with a small number of creators having 10+ videos that made the trending list.

min, max, and histograms for view count, likes, dislikes, and comments:
For View Count:
```{r}
cat('Minimum view count:', min(data$view_count), '\n')
cat('Maximum view count:', max(data$view_count), '\n')
cat('Average view_count:', mean(data$view_count), '\n')

summary(data$view_count)

# Shows the frequency of view count ranges over most of the rows
ggplot(data) + geom_histogram(aes(x=view_count), bins=1500) + coord_cartesian(xlim=c(0,15000000))

# Shows  the frequency of view count ranges over the meat of the curve
ggplot(data) + geom_histogram(aes(x=view_count), bins=6000) + coord_cartesian(xlim=c(0,3000000))

# Shows the log-views distribution
ggplot(data) + geom_histogram(aes(x=log(view_count)), bins=100) 

ggplot(data) + geom_density(aes(x=log(view_count, base=10))) 

```

For Likes:
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)
data_with_ratings



cat('Minimum likes:', min(data_with_ratings$likes), '\n')
cat('Maximum likes:', max(data_with_ratings$likes), '\n')
cat('Average likes:', mean(data_with_ratings$likes), '\n')

summary(data_with_ratings$likes)

# Shows the frequency of likes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=1500) + coord_cartesian(xlim=c(0,2000000))

# Shows  the frequency of likes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=6000) + coord_cartesian(xlim=c(0,300000))

# Shows the log-likes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(likes)), bins=100) 

ggplot(data_with_ratings) + geom_density(aes(x=log(likes, base=10))) 

```

For dislikes
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)



cat('Minimum dislikes:', min(data_with_ratings$dislikes), '\n')
cat('Maximum dislikes:', max(data_with_ratings$dislikes), '\n')
cat('Average dislikes:', mean(data_with_ratings$dislikes), '\n')

summary(data_with_ratings$dislikes)

# Shows the frequency of dislikes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=1500) +  coord_cartesian(xlim=c(0,50000))

# Shows  the frequency of dislikes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=10000) + coord_cartesian(xlim=c(0,5000))

# Shows the log-dislikes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(dislikes)), bins=100) 

ggplot(data_with_ratings) + geom_density(aes(x=log(dislikes, base=10))) 

```

For comment_count
```{r}
# Inspect the number of comments disabled
data %>% group_by(comments_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 1200 for ratings, not that significant.

data_with_comments = data %>% filter(comments_disabled==FALSE)



cat('Minimum comments:', min(data_with_comments$comment_count), '\n')
cat('Maximum comments:', max(data_with_comments$comment_count), '\n')
cat('Average comments:', mean(data_with_comments$comment_count), '\n')

summary(data_with_comments$comment_count)

# Shows the frequency of comments count ranges over most of the rows
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=10000) +  coord_cartesian(xlim=c(0,75000))

# Shows  the frequency of comments count ranges over the meat of the curve
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=20000) + coord_cartesian(xlim=c(0,20000))

# Shows the log-comment count distribution
ggplot(data_with_comments) + geom_histogram(aes(x=log(comment_count)), bins=100) 

ggplot(data_with_comments) + geom_density(aes(x=log(comment_count, base=10))) 

```


Now, looking into the likes/dislikes relationship:
First, I will plot the two as a scatterplot to look for a relationship.
```{r}
# Looks like a few videos with large amounts of each are compressing the rest of the data.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.1)

# Zooming in to see what the meat of the data looks like.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000))

# Trying to fit a line to the data, even though it doesnt look linear.
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000)) + geom_smooth(method=lm)

# Zoomed in further, looks even less linear. .
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.05) + coord_cartesian(xlim=c(0,50000), ylim=c(0,3000)) + geom_smooth(method=lm)
```
Clearly from the graphs, the number of dislikes does not have a strong linear relationship with the number of likes. This means that different videos are different in how much polarization they evoke in the viewership.

**OH NO** Something is wrong with my data. I need to figure out where extra records are coming from.
UPDATE: Figured out where the extra lines were coming from with the above analysis. I need to clean my data more, and will send the above code to the transformation RNotebook to do this. Completed.



Before we go any further, I need to decide whether to cut my dataset out for the first appearance of a video on the list or whether to keep every record for every video. I think a good examination for this would be whether videos normally trend for consecutive days, or whether they appear and disappear repeatedly from the list.

```{r}

data %>% group_by(video_id) %>% summarise(first_day = min(trending_date), 
                                          last_day = max(trending_date),
                                          days_range = as.numeric(difftime(last_day ,first_day , units = c("days")))+1,
                                          total_days = n(),
                                          consecutive_ratio = total_days / days_range) %>%
    ggplot() + geom_histogram(aes(x=consecutive_ratio), bins=12) +coord_cartesian(xlim=c(0,1))
```
From this graph, it is clear to see that the vast majority of the videos that trend for multiple days do so consecutively, or nearly consecutively. I think the majority of my analysis should be done on just the first time a video trends, since that is the real cutoff point. Doing most of the analysis on the full data set would lead to overrepresentation of multi-trenders. 


Now, analyzing the title. I will do work for title length, title word count, title punctuation, title proportion caps, title w/ open/close parenthesis, maybe grouped by days on list (or above avg / below avg days)?

Title Length:
```{r}
data %>% mutate(title_length=nchar(title)) %>% 
    ggplot() + geom_histogram(aes(x=title_length), bins=36)

data %>% mutate(title_length=nchar(title)) %>% summarise(longest_title=max(title_length), shortest_title=min(title_length))


data %>% mutate(title_length=nchar(title)) %>% summarise(median_title_length=median(title_length), avg_title_length=mean(title_length), stdev_title_length=sd(title_length))
```

Title Word Count:
```{r}


word_count = function(string) {
    return(length(strsplit(string, ' ')[[1]]))
}

title_words = sapply(data$title, 'word_count', USE.NAMES = FALSE)

data %>% mutate(title_word_count=sapply(title, 'word_count', USE.NAMES = FALSE)) %>%
    ggplot() + geom_density(aes(x=title_word_count, fill='word_density'), adjust=1.7, alpha=.3)


data %>% mutate(title_word_count=sapply(title, 'word_count', USE.NAMES = FALSE)) %>% summarise(median_title_words=median(title_word_count), avg_title_words=mean(title_word_count), stdev_title_words=sd(title_word_count))
```

Title Proportion Capital Letters:
```{r}
title_caps_ratio = data %>% transmute(title_caps_ratio = str_count(title, "[A-Z]") / 
                    (str_count(title, "[A-Z]") + str_count(title, "[a-z]")))
summary(title_caps_ratio)

data %>% mutate(title_caps_ratio = str_count(title, "[A-Z]") / 
                    (str_count(title, "[A-Z]") + str_count(title, "[a-z]"))) %>% 
    ggplot() + geom_histogram(aes(x=title_caps_ratio), bins=20)
# Average novel has 3-5%, average word in english is 5.1 characters which would be on average .196 if each first letter was capitalized.

```





Moving on from title, I really need to get cracking here.
ChannelId is the next field up. I will do the same analysis for it as for title.


Channel Length:
```{r}
data %>% mutate(channel_length=nchar(channelTitle)) %>% 
    ggplot() + geom_histogram(aes(x=channel_length), bins=20)

data %>% mutate(channel_length=nchar(channelTitle)) %>% summarise(longest_channel=max(channel_length), shortest_channel=min(channel_length))


data %>% mutate(channel_length=nchar(channelTitle)) %>% summarise(median_channel_length=median(channel_length), avg_channel_length=mean(channel_length), stdev_channel_length=sd(channel_length))


data %>% mutate(channel_length=nchar(channelTitle)) %>% 
ggplot() + geom_density(aes(x=channel_length, fill='word_density'), adjust=2, alpha=.3)
```

Channel Word Count:
```{r}


word_count = function(string) {
    return(length(strsplit(string, ' ')[[1]]))
}

channel_words = sapply(data$channelTitle, 'word_count', USE.NAMES = FALSE)

data %>% mutate(channel_word_count=sapply(channelTitle, 'word_count', USE.NAMES = FALSE)) %>%
    ggplot() + geom_bar(aes(x=channel_word_count)) 


data %>% mutate(channel_word_count=sapply(channelTitle, 'word_count', USE.NAMES = FALSE)) %>% summarise(median_channel_words=median(channel_word_count), avg_channel_words=mean(channel_word_count), stdev_channel_words=sd(channel_word_count), longest_channel=max(channel_word_count))
```


Moving on to Category:
Will look at category proportions, category vs. views, and category vs comments. This is one of the only data fields broad enough to pull meaningful info from views/comments.

```{r}
ggplot(data) + geom_bar(aes(x=reorder(category_text, category_text, function(x) length(x)))) + coord_flip()
```

Now, create a df with the aggregated likes/comments values I will be using.
```{r}

category_aggregates = data %>% group_by(category_text) %>% summarise(count=n(), avg_likes=mean(likes), median_likes=median(likes), avg_comments=mean(comment_count), median_comments=median(comment_count), avg_views=mean(view_count), median_views=median(view_count))

category_aggregates
    
    
```

Graph median views for categories:
```{r}
category_aggregates %>%
    ggplot() + geom_col(aes(x=reorder(category_text, median_views), y=median_views)) + coord_flip()

```


Graph median likes for categories:
```{r}
category_aggregates %>%
    ggplot() + geom_col(aes(x=reorder(category_text, median_likes), y=median_likes)) + coord_flip()

```
Graph median comment_count for categories
```{r}
category_aggregates %>% 
    ggplot() + geom_col(aes(x=reorder(category_text, median_comments), y=median_comments)) + coord_flip()
```

Try to put these three charts side by side for comparison:
```{r}
ggplot(category_aggregates, aes(x=reorder(category_text, count))) + geom_col(aes(y=count)) + coord_flip()

```

Median views per category bar chart sorted by frequency of category on trending list:
```{r}
ggplot(category_aggregates, aes(x=reorder(category_text, count))) + geom_col(aes(y=median_views)) + coord_flip()
```

Median likes per category bar chart sorted by frequency of category on trending list:
```{r}
ggplot(category_aggregates, aes(x=reorder(category_text, count))) + geom_col(aes(y=median_likes)) + coord_flip()
```

Median comment_count per category bar chart sorted by frequency of category on trending list:
```{r}
ggplot(category_aggregates, aes(x=reorder(category_text, count))) + geom_col(aes(y=median_comments)) + coord_flip()
```
Rough measure of the most durable categories, how long from creation to trending on average.
```{r}
data %>% group_by(category_text) %>% summarise(avg_days_to_trending = mean(publish_to_trending_days)) %>%
    ggplot() + geom_col(aes(x=reorder(category_text, avg_days_to_trending), y=avg_days_to_trending)) + coord_flip()

```

Another rough measure of the most durable categories, how many days videos trend for on average
```{r}
data %>% group_by(category_text, video_id) %>% summarise(days_trending=n()) %>% summarise(avg_days_trending=mean(days_trending)) %>%
    ggplot() + geom_col(aes(x=reorder(category_text, avg_days_trending), y=avg_days_trending)) + coord_flip()
```
Very surprised that the two above charts didnt show larger deviations. I would have thought categories like sports and news that are very timely would have very short timeframes, whereas videos like music and entertainment that are more durable (don't have a built in expiration date) would have lasted longer. That is evident, but not nearly in the amount i'd think.

Clearly, even though there are lots more music and entertainment vidoes on the trending list, it is harder to make the list (in terms of views) in the over-represented categories. My guess are those are the categories where videos get the most likes overall (this data tells nothing about that). If your goal is just to make the list, you'd want to be in the underrepresented categories (less views to be included), but it is likely much harder to get large numbers of views in those categories. 

NOTE: Categories info is useful to compare to views and comments because it gives an estimate of how many views/likes/comments are needed to get on the list, in the average publish-to-trending timeframe, for each category.


Okay, Done with category. Moving on to... top tags. Yikes.


Tags appearance statistics.
```{r}
cat('There are', length(unique(data$top_tag)), 'unique top_tags in the dataset')

tags_counts = data %>% group_by(top_tag) %>% summarise(count=n()) %>% arrange(desc(count))
tags_counts

data %>% filter(top_tag=='[none]') %>% summarise(median_views_tagless=median(view_count), avg_days_trending_tagless=(n()/length(unique(video_id))))

data %>% filter(top_tag!='[none]') %>% summarise(median_views_tagged=median(view_count), avg_days_trending_tagged=(n()/length(unique(video_id))))

```
Interesting, there is hardly any difference in performance metrics for tagged vs. untagged. But I think the more relevant metric is that untagged videos only make up 7,300 of 56,000 trending videos. Clearly, tag your videos.


Top tags
```{r}
tags_counts %>% head(20)
tags_counts %>% tail(20)

dim(tags_counts %>% filter(count==1))[1]
```
The top tags are pretty general words, which you'd expect, but they still in many cases tell more about the videos in question than the categories. I bet it combines pretty well with the category_text to give you more info about a video.

Interestingly, there are still 34 tags in the database that, despite being the most popular tag for the video in question, were unique to that video. No other trending video in the 280 day period used the tag.


Top 5 tags by category
Creating this chart once instead of 15 times (the facet wrap looked BAAAAAAD), below are all the categories to plug into the code.
"Film & Animation"    "Autos & Vehicles"    "Music"   "Pets & Animals"   "Sports"                "Travel & Events"      "Gaming"                "People & Blogs"        "Comedy"               "Entertainment"         "News & Politics"       "Howto & Style"        "Education"             "Science & Technology"  "Nonprofits & Activism"
```{r}
data %>% group_by(category_text, top_tag) %>% summarise(count=n()) %>% arrange(category_text, desc(count)) %>% slice(1:10) %>% filter(category_text=='Autos & Vehicles') %>%
    ggplot() + geom_col(aes(x=reorder(top_tag, count), y=count)) + coord_flip()

```


Tags to median views required to make the list (dont need comments, likes)
```{r}
tags_aggregates = data %>% group_by(top_tag) %>% summarise( count=n(), median_views=median(view_count), avg_days_to_trend=mean(publish_to_trending_days), average_days_trending=n()/length(unique(video_id)))

tags_aggregates %>% ggplot() + geom_histogram(aes(x=log(median_views)), bins=200) #+ coord_cartesian(xlim=c(0,10000000))

tags_aggregates %>% arrange(desc(median_views)) %>% head(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, median_views), y=median_views)) + coord_flip()

tags_aggregates %>% arrange(desc(median_views)) %>% tail(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, median_views), y=median_views)) + coord_flip()
```
relationship between median views for a tag and how many times it was used for a video?

```{r}
data %>% group_by(top_tag) %>% summarise(occurrences=n(), median_views=median(view_count)) %>%
    ggplot() + geom_point(aes(x=occurrences, y=median_views), alpha=0.4) +coord_cartesian(xlim=c(0,1000))
```
Not sure what to conclude.

Tags creation to trending timeframe
```{r}
tags_aggregates %>% ggplot() + geom_histogram(aes(x=avg_days_to_trend), bins=100) +coord_cartesian(xlim=c(0,10))


tags_aggregates %>% arrange(desc(avg_days_to_trend)) %>% head(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, avg_days_to_trend), y=avg_days_to_trend)) + coord_flip()

tags_aggregates %>% arrange(desc(avg_days_to_trend)) %>% tail(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, avg_days_to_trend), y=avg_days_to_trend)) + coord_flip()
```


Tags average days trending
```{r}
tags_aggregates %>% ggplot() + geom_histogram(aes(x=average_days_trending), bins=15) 


tags_aggregates %>% arrange(desc(average_days_trending)) %>% head(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, average_days_trending), y=average_days_trending)) + coord_flip()

tags_aggregates %>% arrange(desc(average_days_trending)) %>% tail(20) %>% 
    ggplot() + geom_col(aes(x=reorder(top_tag, average_days_trending), y=average_days_trending)) + coord_flip()
```

Okay, I think that about sums it up for EDA. Now to decide what to show for my project and start building the RShiny app!



































