---
title: "Exploratory Data Analysis"
#output: html_notebook
---
```{r}
library(dplyr)
library(ggplot2)
library(stringr)
```

This notebook will focus on Exploratory Data Analysis to help understand my data more and see emerging areas of trends and relationships. 
Load in the processed data:
```{r}
data = read.csv('data\\Youtube_Working_Dataset_NEW.csv', header=TRUE)
data
```
How many videos per day made the trending list?
```{r}
# Quick summary of the number of rows per trending day
summary(data %>% count(trending_date))

# Shows the number of trending days that had each count of row numbers.
data %>% count(trending_date) %>% count(n)
```
Almost all the days covered by the dataset had 199 or 200 entries for that day. The 199 and 197 entry days are close enough to leave as-is, and I fixed the issue of a few days that had 400.

```{r}
dim(data)[1] - dim(distinct(data))[1]
```

How many different videos made the list altogether?
```{r}
unique_vids = dim(distinct(data, video_id))[1]
unique_vids
```
9,835 different videos made the top daily trending videos list in the 280 days covered by the data.
```{r}
dim(data)[1] / unique_vids
```
This means that on average, a video that made the trending list spent 5.69 days trending in the last 280 days.


Now, how many videos were repeat trenders?
```{r}
data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1)
unique_vids - dim(data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1))[1]

```
According to this quick analysis, there are only 376 out of 9,835 videos that made the top daily trending list only once. That in itself is both very surprising and not that surprising... If a video makes the trending list on a day it is very likely to make it again.

Plot the frequency of trending days. I want to see what is a common amount of time to trend for. 
```{r}
trend_days_count = data %>% count(video_id) %>% arrange(desc(n))
trend_days_proportions = data %>% count(video_id) %>% count(n) %>% mutate(proportion=round(nn/unique_vids,3)) %>% select(n, proportion) 

trend_days_proportions %>% ggplot() + geom_col(aes(x=n, y=proportion))
cat('Standard Deviation of days trending:', sd(trend_days_count$n), '\n')
cat('Mean days trending:', mean(trend_days_count$n), '\n')
cat('Median days trending:', median(trend_days_count$n), '\n')
cat('Max days trending:', max(trend_days_count$n), '\n')
```
Different line of questioning: How many creators got multiple videos on the trending list in the 280-day period?
```{r}
vids_per_creator = data %>% group_by(channelId) %>% 
    summarise(number_trending_videos=n_distinct(video_id))
creator_count = dim(vids_per_creator)[1]
cat('Number of creators who made the top daily trending list in 283 days:', creator_count, '\n')
cat('Number of creators who had more than one video make the top trending list:',
    dim(vids_per_creator[vids_per_creator$number_trending_videos > 1,])[1], '\n')
cat('Average videos per creator on list:', mean(vids_per_creator$number_trending_videos), '\n')
cat('Median videos per creator on list:', median(vids_per_creator$number_trending_videos), '\n')

vids_per_creator %>% ggplot() + geom_bar(aes(x=number_trending_videos))
```
About half of all creators who made the list only had 1 video make the top daily trending list during this time period.
The data is very skewed, with a small number of creators having 10+ videos that made the trending list.

min, max, and histograms for view count, likes, dislikes, and comments:
For View Count:
```{r}
cat('Minimum view count:', min(data$view_count), '\n')
cat('Maximum view count:', max(data$view_count), '\n')
cat('Average view_count:', mean(data$view_count), '\n')

summary(data$view_count)

# Shows the frequency of view count ranges over most of the rows
ggplot(data) + geom_histogram(aes(x=view_count), bins=1500) + coord_cartesian(xlim=c(0,15000000))

# Shows  the frequency of view count ranges over the meat of the curve
ggplot(data) + geom_histogram(aes(x=view_count), bins=6000) + coord_cartesian(xlim=c(0,3000000))

# Shows the log-views distribution
ggplot(data) + geom_histogram(aes(x=log(view_count)), bins=100) 

ggplot(data) + geom_histogram(aes(x=log(view_count, base=10)), bins=100) 

```

For Likes:
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)
data_with_ratings



cat('Minimum likes:', min(data_with_ratings$likes), '\n')
cat('Maximum likes:', max(data_with_ratings$likes), '\n')
cat('Average likes:', mean(data_with_ratings$likes), '\n')

summary(data_with_ratings$likes)

# Shows the frequency of likes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=1500) + coord_cartesian(xlim=c(0,2000000))

# Shows  the frequency of likes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=6000) + coord_cartesian(xlim=c(0,300000))

# Shows the log-likes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(likes)), bins=100) 

ggplot(data_with_ratings) + geom_histogram(aes(x=log(likes, base=10)), bins=100) 

```

For dislikes
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)



cat('Minimum dislikes:', min(data_with_ratings$dislikes), '\n')
cat('Maximum dislikes:', max(data_with_ratings$dislikes), '\n')
cat('Average dislikes:', mean(data_with_ratings$dislikes), '\n')

summary(data_with_ratings$dislikes)

# Shows the frequency of dislikes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=1500) +  coord_cartesian(xlim=c(0,50000))

# Shows  the frequency of dislikes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=10000) + coord_cartesian(xlim=c(0,5000))

# Shows the log-dislikes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(dislikes)), bins=100) 

ggplot(data_with_ratings) + geom_histogram(aes(x=log(dislikes, base=10)), bins=100) 

```

For comment_count
```{r}
# Inspect the number of comments disabled
data %>% group_by(comments_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 1200 for ratings, not that significant.

data_with_comments = data %>% filter(comments_disabled==FALSE)



cat('Minimum comments:', min(data_with_comments$comment_count), '\n')
cat('Maximum comments:', max(data_with_comments$comment_count), '\n')
cat('Average comments:', mean(data_with_comments$comment_count), '\n')

summary(data_with_comments$comment_count)

# Shows the frequency of comments count ranges over most of the rows
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=10000) +  coord_cartesian(xlim=c(0,75000))

# Shows  the frequency of comments count ranges over the meat of the curve
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=20000) + coord_cartesian(xlim=c(0,20000))

# Shows the log-comment count distribution
ggplot(data_with_comments) + geom_histogram(aes(x=log(comment_count)), bins=100) 

ggplot(data_with_comments) + geom_histogram(aes(x=log(comment_count, base=10)), bins=100) 

```


Now, looking into the likes/dislikes relationship:
First, I will plot the two as a scatterplot to look for a relationship.
```{r}
# Looks like a few videos with large amounts of each are compressing the rest of the data.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.1)

# Zooming in to see what the meat of the data looks like.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000))

# Trying to fit a line to the data, even though it doesnt look linear.
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000)) + geom_smooth(method=lm)

# Zoomed in further, looks even less linear. .
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.05) + coord_cartesian(xlim=c(0,50000), ylim=c(0,3000)) + geom_smooth(method=lm)
```
Clearly from the graphs, the number of dislikes does not have a strong linear relationship with the number of likes. This means that different videos are different in how much polarization they evoke in the viewership.

**OH NO** Something is wrong with my data. I need to figure out where extra records are coming from.
UPDATE: Figured out where the extra lines were coming from with the above analysis. I need to clean my data more, and will send the above code to the transformation RNotebook to do this. Completed.



Before we go any further, I need to decide whether to cut my dataset out for the first appearance of a video on the list or whether to keep every record for every video. I think a good examination for this would be whether videos normally trend for consecutive days, or whether they appear and disappear repeatedly from the list.

```{r}

data %>% group_by(video_id) %>% summarise(first_day = min(trending_date), 
                                          last_day = max(trending_date),
                                          days_range = as.numeric(difftime(last_day ,first_day , units = c("days")))+1,
                                          total_days = n(),
                                          consecutive_ratio = total_days / days_range) %>%
    ggplot() + geom_histogram(aes(x=consecutive_ratio), bins=12) +coord_cartesian(xlim=c(0,1))
```
From this graph, it is clear to see that the vast majority of the videos that trend for multiple days do so consecutively, or nearly consecutively. I think the majority of my analysis should be done on just the first time a video trends, since that is the real cutoff point. Doing most of the analysis on the full data set would lead to overrepresentation of multi-trenders. 


Now, analyzing the title. I will do work for title length, title word count, title punctuation, title proportion caps, title w/ open/close parenthesis, maybe grouped by days on list (or above avg / below avg days)?

Title Length:
```{r}
data %>% mutate(title_length=nchar(title)) %>% 
    ggplot() + geom_histogram(aes(x=title_length), bins=36)

data %>% mutate(title_length=nchar(title)) %>% summarise(longest_title=max(title_length), shortest_title=min(title_length))


data %>% mutate(title_length=nchar(title)) %>% summarise(median_title_length=median(title_length), avg_title_length=mean(title_length), stdev_title_length=sd(title_length))
```

Title Word Count:
```{r}


word_count = function(string) {
    return(length(strsplit(string, ' ')[[1]]))
}

title_words = sapply(data$title, 'word_count', USE.NAMES = FALSE)

data %>% mutate(title_word_count=sapply(title, 'word_count', USE.NAMES = FALSE)) %>%
    ggplot() + geom_density(aes(x=title_word_count, fill='word_density'), adjust=1.7, alpha=.3)


data %>% mutate(title_word_count=sapply(title, 'word_count', USE.NAMES = FALSE)) %>% summarise(median_title_words=median(title_word_count), avg_title_words=mean(title_word_count), stdev_title_words=sd(title_word_count))
```

Title Proportion Capital Letters:
```{r}
title_caps_ratio = data %>% transmute(title_caps_ratio = str_count(title, "[A-Z]") / 
                    (str_count(title, "[A-Z]") + str_count(title, "[a-z]")))
summary(title_caps_ratio)

data %>% mutate(title_caps_ratio = str_count(title, "[A-Z]") / 
                    (str_count(title, "[A-Z]") + str_count(title, "[a-z]"))) %>% 
    ggplot() + geom_histogram(aes(x=title_caps_ratio), bins=20)
# Average novel has 3-5%, average word in english is 5.1 characters which would be on average .196 if each first letter was capitalized.

```





Moving on from title, I really need to get cracking here.
ChannelId is the next field up. I will do the same analysis for it as for title.


Channel Length:
```{r}
data %>% mutate(channel_length=nchar(channelTitle)) %>% 
    ggplot() + geom_histogram(aes(x=channel_length), bins=20)
data %>% mutate(channel_length=nchar(channelTitle)) %>% summarise(longest_channel=max(channel_length), shortest_channel=min(channel_length))


data %>% mutate(channel_length=nchar(channelTitle)) %>% summarise(median_channel_length=median(channel_length), avg_channel_length=mean(channel_length), stdev_channel_length=sd(channel_length))


data %>% mutate(channel_length=nchar(channelTitle)) %>% 
ggplot() + geom_density(aes(x=channel_length, fill='word_density'), adjust=2, alpha=.3)
```

Channel Word Count:
```{r}


word_count = function(string) {
    return(length(strsplit(string, ' ')[[1]]))
}

channel_words = sapply(data$channelTitle, 'word_count', USE.NAMES = FALSE)

data %>% mutate(channel_word_count=sapply(channelTitle, 'word_count', USE.NAMES = FALSE)) %>%
    ggplot() + geom_bar(aes(x=channel_word_count)) 


data %>% mutate(channel_word_count=sapply(channelTitle, 'word_count', USE.NAMES = FALSE)) %>% summarise(median_channel_words=median(channel_word_count), avg_channel_words=mean(channel_word_count), stdev_channel_words=sd(channel_word_count))
```


Moving on to Category:
Will look at category proportions, category vs. views, and category vs comments. This is one of the only data fields broad enough to pull meaningful info from views/comments.

```{r}
ggplot(data) + geom_bar(aes(x=reorder(category_text, category_text, function(x) length(x)))) + coord_flip()
```

Now, category by average likes:
```{r}


```
















































