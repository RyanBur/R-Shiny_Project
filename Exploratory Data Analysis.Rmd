---
title: "Exploratory Data Analysis"
#output: html_notebook
---
```{r}
library(dplyr)
library(ggplot2)
```

This notebook will focus on Exploratory Data Analysis to help understand my data more and see emerging areas of trends and relationships. 
Load in the processed data:
```{r}
data = read.csv('data\\Youtube_Working_Dataset.csv', header=TRUE)
data
```
How many videos per day made the trending list?
```{r}
# Quick summary of the number of rows per trending day
summary(data %>% count(trending_date))

# Shows the number of trending days that had each count of row numbers.
data %>% count(trending_date) %>% count(n)
```
Almost all the days covered by the dataset (272 / 283) had 200 entries for that day. The 199 and 197 entry days are close enough to leave as-is, but the days with 400 entries lead me to believe these days were double-entered into the database. If the distinct rows in the dataset is 4*(400-200) = 800 shorter than the dataset itself, I will be pretty sure this is true. 
```{r}
dim(data)[1] - dim(distinct(data))[1]
```
This does not appear to be the case, entries are not being repeated in a systematic manner, so I will leave the dataset as-is. If the analysis was more in-depth and had greater time I would investigate this further by looking into the specifics of the days that had 400 entries.

Conclusion is that majority of days have 200 trending entries per day, with a few having just under and a few having double.

How many different videos made the list altogether?
```{r}
unique_vids = dim(distinct(data, video_id))[1]
unique_vids
```
9,954 different videos made the top daily trending videos list in the 283 days covered by the data.
```{r}
dim(data)[1] / unique_vids
```
This means that on average, a video that made the trending list spent 5.77 days trending in the last 283 days.


Now, how many videos were repeat trenders?
```{r}
data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1)
unique_vids - dim(data %>% count(video_id) %>% arrange(desc(n)) %>% filter(n>1))[1]

```
According to this quick analysis, there are only 249 out of 9,954 videos that made the top daily trending list only once. That in itself is both very surprising and not that surprising... If a video makes the trending list on a day it is very likely to make it again.

Plot the frequency of trending days. I want to see what is a common amount of time to trend for. 
```{r}
trend_days_count = data %>% count(video_id) %>% arrange(desc(n))
trend_days_proportions = data %>% count(video_id) %>% count(n) %>% mutate(proportion=round(nn/unique_vids,3)) %>% select(n, proportion) 

trend_days_proportions %>% ggplot() + geom_col(aes(x=n, y=proportion))
cat('Standard Deviation of days trending:', sd(trend_days_count$n), '\n')
cat('Mean days trending:', mean(trend_days_count$n), '\n')
cat('Median days trending:', median(trend_days_count$n), '\n')
cat('Max days trending:', max(trend_days_count$n), '\n')
```
Different line of questioning: How many creators got multiple videos on the trending list in the 283-day period?
```{r}
vids_per_creator = data %>% group_by(channelId) %>% 
    summarise(number_trending_videos=n_distinct(video_id))
creator_count = dim(vids_per_creator)[1]
cat('Number of creators who made the top daily trending list in 283 days:', creator_count, '\n')
cat('Number of creators who had more than one video make the top trending list:',
    dim(vids_per_creator[vids_per_creator$number_trending_videos > 1,])[1], '\n')
cat('Average videos per creator on list:', mean(vids_per_creator$number_trending_videos), '\n')
cat('Median videos per creator on list:', median(vids_per_creator$number_trending_videos), '\n')

vids_per_creator %>% ggplot() + geom_bar(aes(x=number_trending_videos))
```
About half of all creators who made the list only had 1 video make the top daily trending list during this time period.
The data is very skewed, with a small number of creators having 10+ videos that made the trending list.

min, max, and histograms for view count, likes, dislikes, and comments:
For View Count:
```{r}
cat('Minimum view count:', min(data$view_count), '\n')
cat('Maximum view count:', max(data$view_count), '\n')
cat('Average view_count:', mean(data$view_count), '\n')

summary(data$view_count)

# Shows the frequency of view count ranges over most of the rows
ggplot(data) + geom_histogram(aes(x=view_count), bins=1500) + coord_cartesian(xlim=c(0,15000000))

# Shows  the frequency of view count ranges over the meat of the curve
ggplot(data) + geom_histogram(aes(x=view_count), bins=6000) + coord_cartesian(xlim=c(0,3000000))

# Shows the log-views distribution
ggplot(data) + geom_histogram(aes(x=log(view_count)), bins=100) 

ggplot(data) + geom_histogram(aes(x=log(view_count, base=10)), bins=100) 

```

For Likes:
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)



cat('Minimum likes:', min(data_with_ratings$likes), '\n')
cat('Maximum likes:', max(data_with_ratings$likes), '\n')
cat('Average likes:', mean(data_with_ratings$likes), '\n')

summary(data_with_ratings$likes)

# Shows the frequency of likes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=1500) + coord_cartesian(xlim=c(0,2000000))

# Shows  the frequency of likes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=likes), bins=6000) + coord_cartesian(xlim=c(0,300000))

# Shows the log-likes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(likes)), bins=100) 

ggplot(data_with_ratings) + geom_histogram(aes(x=log(likes, base=10)), bins=100) 

```

For dislikes
```{r}
# Inspect the number of ratings disabled
data %>% group_by(ratings_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 300 for ratings, not that significant.

data_with_ratings = data %>% filter(ratings_disabled==FALSE)



cat('Minimum dislikes:', min(data_with_ratings$dislikes), '\n')
cat('Maximum dislikes:', max(data_with_ratings$dislikes), '\n')
cat('Average dislikes:', mean(data_with_ratings$dislikes), '\n')

summary(data_with_ratings$dislikes)

# Shows the frequency of dislikes ranges over most of the rows
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=1500) +  coord_cartesian(xlim=c(0,50000))

# Shows  the frequency of dislikes ranges over the meat of the curve
ggplot(data_with_ratings) + geom_histogram(aes(x=dislikes), bins=10000) + coord_cartesian(xlim=c(0,5000))

# Shows the log-dislikes distribution
ggplot(data_with_ratings) + geom_histogram(aes(x=log(dislikes)), bins=100) 

ggplot(data_with_ratings) + geom_histogram(aes(x=log(dislikes, base=10)), bins=100) 

```

For comment_count
```{r}
# Inspect the number of comments disabled
data %>% group_by(comments_disabled) %>% summarise(total=n())
# These show that getting rid of the disabled ones reduces the row count by 1200 for ratings, not that significant.

data_with_comments = data %>% filter(comments_disabled==FALSE)



cat('Minimum comments:', min(data_with_comments$comment_count), '\n')
cat('Maximum comments:', max(data_with_comments$comment_count), '\n')
cat('Average comments:', mean(data_with_comments$comment_count), '\n')

summary(data_with_comments$comment_count)

# Shows the frequency of comments count ranges over most of the rows
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=10000) +  coord_cartesian(xlim=c(0,75000))

# Shows  the frequency of comments count ranges over the meat of the curve
ggplot(data_with_comments) + geom_histogram(aes(x=comment_count), bins=20000) + coord_cartesian(xlim=c(0,20000))

# Shows the log-comment count distribution
ggplot(data_with_comments) + geom_histogram(aes(x=log(comment_count)), bins=100) 

ggplot(data_with_comments) + geom_histogram(aes(x=log(comment_count, base=10)), bins=100) 

```


Now, looking into the likes/dislikes relationship:
First, I will plot the two as a scatterplot to look for a relationship.
```{r}
# Looks like a few videos with large amounts of each are compressing the rest of the data.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.1)

# Zooming in to see what the meat of the data looks like.
ggplot(data_with_ratings) + geom_point(aes(x=likes, y=dislikes), alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000))

# Trying to fit a line to the data, even though it doesnt look linear.
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.01) + coord_cartesian(xlim=c(0,300000), ylim=c(0,25000)) + geom_smooth(method=lm)

# Zoomed in further, looks even less linear. .
ggplot(data_with_ratings, aes(x=likes, y=dislikes)) + geom_point(alpha=0.05) + coord_cartesian(xlim=c(0,50000), ylim=c(0,3000)) + geom_smooth(method=lm)
```
Clearly from the graphs, the number of dislikes does not have a strong linear relationship with the number of likes. This means that different videos are different in how much polarization they evoke in the viewership.










